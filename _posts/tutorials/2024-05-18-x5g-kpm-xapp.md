---
layout: post
title: "KPM xApp with the OSC Near-RT RIC and NVIDIA ARC on X5G"
date: 2024-05-18
category: tutorials
author:
short-description: How to run a KPM xApp leveraging the X5G testbed comprising OAI, NVIDIA ARC-OTA, and the OSC Near-RT RIC
---

{% assign publications = site.data.publications %}

{% assign el = publications.other | where: "name", "villa2024x5g" %}
{% assign element = el[0] %}
{% capture pub_villa2024x5g %}
{% include pub-template.html %}
{% endcapture %}

[X5G](https://openrangym.com/experimental-platforms/x5g) is an open, programmable, and multi-vendor private 5G O-RAN testbed that leverages the [NVIDIA Aerial](https://docs.nvidia.com/aerial/index.html) SDK for the High-PHY layer and [OpenAirInterface](https://openairinterface.org/) for the higher layers.

More details can be found in the following paper:
> {{ pub_villa2024x5g | strip_newlines }}
> {: .text-justify}

This tutorial servers as a reference on how to operate a Key Performance Measurement (KPM) xApp in an end-to-end platform with similar capabilities to those provided by X5G. A possible final outcome of this tutorial can be found on this [X5G Overview Video](https://www.youtube.com/watch?v=_bYY12wuhzk&ab_channel=MichelePolese).


## Prerequisites
The following prerequisites are recommended to be met in order to properly run the tutorial.

### NVIDIA Aerial
The NVIDIA Aerial information and installation guide are available on its [official website](https://docs.nvidia.com/aerial/cuda-accelerated-ran/index.html).

### OSC Near-RT RIC
The O-RAN Software Community (OSC) Near-RT RIC Release E can be installed following [this guide](https://docs.o-ran-sc.org/projects/o-ran-sc-ric-plt-ric-dep/en/latest/installation-guides.html). Note that for this tutorial, the OSC Near-RT RIC runs on an OpenShift cluster, whereas a plain Kubernetes deployment might require specific networking configurations.

### OpenAirInterface with Custom E2 Agent
{% assign publications = site.data.publications %}

{% assign el = publications.publications | where: "name", "moro2023open" %}
{% assign element = el[0] %}
{% capture pub_moro2023open %}
{% include pub-template.html %}
{% endcapture %}

The OpenAirInterface (OAI) gNB code requires the addition of a custom E2 Agent to manage E2SM (E2 Service Model) functionalities that has been developed and tested as part of the following publication:
> {{ pub_moro2023open | strip_newlines }}
> {: .text-justify}

The E2 agent is based on the OSC e2sim project. An OAI version with the custom E2 Agent already integrated in the [2024.w18](https://gitlab.eurecom.fr/oai/openairinterface5g/-/tags/2024.w18) release of OAI can be found at [this GitHub repository](https://github.com/wineslab/x5g-openairinterface/tree/2024w18_e2agent).

### xApp
A basic and easily extensible xApp developed in Python can be found at the [GitHub repository](https://github.com/wineslab/xapp-oai/tree/kpm-xapp). This repository contains some Python examples that can subscribe to selected RAN parameters, receive periodic indication messages, and send control requests to update these parameters.

### xApp SM Connector
The xApp SM connector component connects the xApp with the OSC Near-RT RIC. On one side, it receives the custom SM buffers to be encapsulated in E2AP (E2 Application Protocol) messages and sent to the RIC. On the other side, it retrieves the custom SM buffers from E2AP to be sent to the xApp. The connector is part of the base xApp code and can be found at [this GitHub repository](https://github.com/wineslab/xapp-oai/tree/kpm-xapp).

### e2sim
The e2sim is a component that can run on the same server as the OAI L2/L3 and ARC PHY and has the duty to encapsulate/decapsulate the custom Service Model (SM) buffers to be sent to or received from the gNB. It communicates with the gNB via UDP sockets. The e2sim codebase can be found at [this GitHub repository](https://github.com/wineslab/ran-e2sim/tree/x5g-e2sim).


## Run the System

We assume that all components listed in the prerequisites, such as the gNB with NVIDIA ARC-OTA and OAI, OSC Near-RT RIC, e2sim, and xApp, have been compiled and are functioning properly. Please note that the commands and terminals used in this tutorial may vary depending on your setup.

To run the system in this tutorial, we open 5 terminals as follows:
- Terminal 1 (T1): NVIDIA L1 cuBB (Server 1 with A100 GPU)
- Terminal 2 (T2): OAI L2 (Server 1 same as cuBB)
- Terminal 3 (T3): e2sim (Server 1 same as cuBB)
- Terminal 4 (T4): xApp connector (xApp Container in OSC RIC)
- Terminal 5 (T5): xApp Python (xApp Container in OSC RIC)
- (Optional) Terminal 6 (T6): UEs

In T1, we first run the `NVIDIA L1 cuBB`, for example, by using the following command (it may vary based on your ARC release and configurations, please refer to [Running cuBB End-to End](https://docs.nvidia.com/aerial/cuda-accelerated-ran/aerial_cubb/cubb_quickstart/running_cubb-end-to-end.html)):

{% highlight bash %}
sudo -E /opt/nvidia/cuBB/build/cuPHY-CP/cuphycontroller/examples/cuphycontroller_scf P5G FXN
{% endhighlight %}

In T2, once the L1 is ready, we can run the custom version of `OAI` with the usual OAI start command (note that this command can also vary based on your OAI configuration):

{% highlight bash %}
./nr-softmodem -O ../../../targets/PROJECTS/GENERIC-NR-5GC/CONF/vnf.sa.band78.fr1.273PRB.Aerial.conf
{% endhighlight %}

From this point onwards, we can connect the UEs as desired, for example, from within T6.

In T3, we run the `e2sim` on the same machine where OAI is running, specifying the IP and port of the OSC Near-RT RIC, for example:

{% highlight bash %}
./run_e2sim.sh 10.100.1.101 32222
{% endhighlight %}

The E2 Agent of the gNB and the RIC are successfully connected if the following prompt is shown:

{% highlight bash %}
[E2AP] Received SETUP-RESPONSE-SUCCESS
{% endhighlight %}

In T4, we can start connecting our xApp by running the `xapp-sm-connector` with:

{% highlight bash %}
cd xapp-sm-connector
./run_connector.sh
{% endhighlight %}

We wait for the connector to finish its initialization, which is indicated by, for example, the following line:

{% highlight bash %}
Opened control socket server on port 7000
{% endhighlight %}

In T5, we start the `xApp` Python code by running, for example:

{% highlight bash %}
cd base-xapp
./run_kpm_xapp.sh
{% endhighlight %}

If everything is running correctly, we should periodically see the UEs' metrics displayed on the screen:

{% highlight bash %}
report index 3
rnti: 15026
tbs_avg_dl: 0.0
tbs_avg_ul: 0.0
is_GBR: false
dl_mac_buffer_occupation: 1953239.0
avg_prbs_dl: 0.0
mcs: 23
avg_tbs_per_prb_dl: 0.0
avg_rsrp: -79.0
ph: 52.0
pcmax: 21.0
dl_total_bytes: 6613826048.0
dl_errors: 0.0
dl_bler: 0.07657613605260849
dl_mcs: 23.0
ul_total_bytes: 7838225.0
ul_errors: 0.0
ul_bler: 2.0877508055683336e-10
ul_mcs: 18.0
{% endhighlight %}

The `kpm-xapp.py` script can be easily modified to save these metrics in an external dB, such as [InfluxDB](https://docs.influxdata.com/influxdb/v2/install/?t=Docker#start-and-configure-influxdb), and then display them on a dashboard, such as [Grafana](https://grafana.com/docs/grafana/v9.3/getting-started/build-first-dashboard/).